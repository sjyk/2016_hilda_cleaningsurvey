\section{Discussion and Future Directions}\label{sec:future}


\begin{figure}[ht]
  \centering
  \includegraphics[width=.95\columnwidth]{datafigs/arch}
  \caption{Current and proposed iterative feedback loop for data cleaning.  
  The top shows the current slow feedback loop, where the data cleaning (magenta) and analysis (robot) steps are split between I.T. and data scientists, and feedback is obtained from the data scientists. 
  The bottom shows the potential unified approach, where the cleaning can be inspected or modified by any user at any step throughout the pipeline.  This can happen through manual changes or auto-tuned optimizations.
  }
  \label{f:arch}
\end{figure}

Our survey results highlighted several bottlenecks that add friction to the data cleaning and analysis process.
The primary finding reiterates the observation that data cleaning is highly contextual---users intimately familiar with the downstream applications are needed to direct the data cleaning process.  In other words, these domain experts are the most important ``humans in the data cleaning loop''.  

In practice, however, we found that the data cleaning process is commonly split across multiple organizational units: the IT department performs data cleaning, and sends the processed data to application developers and data scientists, who in turn perform additional data transformation and analysis (top of Figure~\ref{f:arch}).  This separation inhibits the ability to experiment and test different cleaning procedures and tune their parameters---feedback about the data cleaning process is often delayed until the downstream application developer (e.g., visually) inspects the application results.  We believe that these limitations need not exist, and envision a highly interactive data cleaning and analysis process, wherein infrastructure engineers, data analysts and domain experts can design, evaluate, and modify (with automated support) any stage of the data cleaning workflow (bottom of Figure~\ref{f:arch}). To this end, we present a series of technical challenges spanning HCI, statistic, and data management that must be overcome in order to support a truly interactive data cleaning system.

% As highlighted in the top half of Figure~\ref{f:arch}, changes to the cleaning process occur based on observations made at the end of the analysis pipeline.  

\vspace{0.5em}
\noindent\textbf{Developing High-level Language for Domain Experts:} Data cleaning is an involved process that involves extraction, schema/ontology matching, value imputation, deduplication, and other processes.
In addition, each of these operations encapsulates dozens of specialized algorithms such as machine learning, clustering, or rule-based procedures.
It is both difficult for domain experts to navigate through the zoo of options, and easy for those implementing data cleaning operations to become married to a specific algorithmic choice.
In addition, these parties must interact, and it is important to facilitate the coordination between the two.
There is a need for a high level language for domain experts to describe the data cleaning goals at a logical level (e.g., providing deduplication examples, descriptions of outliers) that also enables physical implementation choices to be guided by automated tools or the technical experts that are tasked with implementing data cleaning at scale. {\color{red}{more details?}} \dhaas{Can we just cite AJAX / Wisteria as "initial approaches", and leave it open-ended?}

\vspace{0.5em}
\noindent\textbf{Usability and Interactivity:} The need to focus on usability and visual interaction has been reiterated across many domains:  Wrangler~\cite{kandel2011wrangler} enables domain experts to perform complex text extraction tasks at scale, and  Polaris~\cite{stolte2002polaris} helps business analysts perform data-cube analysis through a visual interface. These systems place emphasis on the end-to-end process by reducing bottlenecks that stem from human interaction and decision making.    We must similarly lift a high level cleaning language into the interactive domain~\cite{heer2015predictive} in order to tighten the feedback loop between the user and cleaning process.

% Often times the domain experts are not techincal experts\footnote{Although our user study population is biased towards technical users, other studies~\cite{kandel2012} have shown that data analysts are often non-technical domain experts.}, and there is a need to map the high-level language into the visual interactive domain.  We have seen success from intergrating a domain specific language and visual interactions  in specialized data cleaning domains such as text extraction~\cite{kandel2011wrangler}.  Similarly agreementmaker work.
% 
% long pipeline from cleaning and output, so insert inspection.  
\vspace{0.5em}
\noindent\textbf{Application-oriented Cleaning:}  A recurring theme amongst our survey participants was the observation that data cleaning is driven by the needs of the downstream application.  We found surprisingly little evidence of data cleaning as a discrete and isolated process. Systems such as SampleClean~\cite{DBLP:journals/debu/KrishnanWFGKM015} and ActiveClean~\cite{activecleanarxiv} have already shown the potential for leveraging application knowledge to reduce data cleaning costs by {\color{red}{an order of magnitude or more}}.  However these systems have been tailored to specialized use cases (individual aggregation queries and convex models, respectively), and support for other more complex operations as well as multi-stage sequences of analyses is needed.

\vspace{0.5em}
\noindent\textbf{Human-Computer Symbiosis:}   Some participants described tweaking cleaning operations and running the downstream analysis in order to visually inspect the results.  However, this form of manual configuration and parameter tuning does not make the best use of the domain expert's resources.   There is potential to introduce automation, and we have already seen examples of this.  For instance, active learning is used as part of crowd-sourced label acquisition~\cite{gokhale2014corleone,DBLP:journals/pvldb/HaasW0F15} to optimize an operation such as deduplication;  Wrangler automatically generates string extraction rules so that the user only needs to pick from a set of options; and TuPAQ~\cite{sparks2015automating} performs automatic hyperparameter tuning for machine learning models.   There is similar opportunity to identify additional data cleaning operations that can be automatically tuned, as well as to propose modifications to the sequence of operations itself~\cite{wisteria}.  The ultimate goal is to {\it let experts do what they do best, while machines do the rest}. \dhaas{Adam would be proud, but can we really get away with this?}

\vspace{0.5em}
\noindent\textbf{Testing and Debugging:} In order to develop automated optimization and tuning procedures, there must be a metric to optimize.  This can be quite challenging---one common measure of data cleaning effectiveness among survey participants was simply whether or not the downstream process compiled and ran!
This clearly falls short of the standards needed for sophisticated automation, and methods for introducing metrics throughout the cleaning and analysis process are needed.
For example, one might use performance on gold examples of known clean data to evaluate a cleaning operator.
Such data could be acquired up front, or adaptively from a domain expert or crowd worker as cleaning proceeds.

In addition, as data-cleaning researchers, there is a need for a ``dirtiness'' benchmark analgous to industry standard transaction and analytical data processing benchmarks.
Existing systems are often evaluated on synthetically generated errors that are may not reflect reality or on application-specific errors that are too specialized to serve as a standard benchmark across systems.

%Such a benchmark needs to support cases when there is and is not a ground truth dataset.
\vspace{0.5em}
\noindent\textbf{Combating Over-fitting:}
Despite their importance, data cleaning procedures are often under-reported
when presenting analytics results. This is problematic since the data
cleaning operations have a potential to introduce analyst biases,
i.e., favoring a certain outcomes, into the analysis process.
This sentiment is corroborated by our survey results and the results of Kandel et al.~\cite{kandel2012}.
In a sense, this problem is analogous to statistical over-fitting, where if strong assumptions are made on a small sample of data, these transformations may not apply to future evolutions of the application.
An important challenge is designing data cleaning tools that: (1) allow analysts to communicate assumptions (e.g., which records have been removed) when presenting results, (2) automatically determine when an assumption is risky (e.g., correlates with the tested hypothesis), and (3) manages a ``paper trail'' of data transformations.


\section{Conclusion}
In this paper, we have presented a study of industry users of data analysis software that confirms the recent shift in data cleaning processes towards iterative workflows.
Our survey results highlight the issues that frustrate current workflows, and motivate our proposal of the research challenges central to the design of unified systems that can alleviate these issues.
In summary, though we are in the early days of highly interactive data cleaning and preparation, we are optimistic about opportunities for systems that facilitate and automate rapid human-in-the-loop interactivity.


\if{0}
 
* A language or languages for data cleaning.  An operation, such as deduplication involves a large number of possible algorithms or sytems techinques, ranging from crowdsourced solsutions(REF Anhai), machine learning-based fuzzy matching(REF).  It is often easier for a user to provide correct results rather than.
* Be careful about survey biased towards technical people, not those that use GUIs. 
* Usability: Tied with the language are a set of interactions to match the language operations (REF joe’s CIDR paper).  This is admittedly an amorpheous topic.  (REF agreementmaker work)
* Goal oriented cleaning.  General data cleaning is rarely needed, and pay-as-you-go solutions(REF data spaces) are needed in most applications.   This begs the qusetion for hminimizing expert involvement.   Solutions such as sampleclean are oriented towards specializing the human effort towards porudictive tasks that maximize the cleaning for a particular application.  
* Automation. 
    * Hyperparameter tuning, however visual inspection by manually tuning each or combinations of parameters is both slow and and error-prone.  Automated hyperparemeter tuning (REF evan)
    * String value munging, either deduplication, extraction, fixes.  Existing tools are often not sufficient and user intervention (REF Wrangler) is necessary.  However even in this settings, ways to generate suggestions and show them is better than users picking operations to perform.
    * active learning is used, but only for crowd workers that fit inside an operator good progress folks.
    * Opporutinity let the expert be the human in the loop.  The data analyst IS the human in the loop.
    * Let humans do what they do best, machines to the rest.
* Testing and debugging cleaning.  Data cleaning does not live within a vacuum.  It is often motivated by a set of desired applications, and .  How can it be tested?  For example, what effects will introducing or modiying a data cleaning procedure have on the rest of the dataset, or the downstream application?  For example, in our survey several respondents noted that their evaluation was based on whether the downstream applicaiton compiles or not.
    * Reference Self eval idea.  long pipeline from cleaning and output, so insert inspection.  
    * Reference benchmarking work and systematic ways to evaluate the efficacy of an approach when there is ground truth and when there is no ground truth.
* (SANJAY) Overfitting and sharing.   It is easy to overfit the data cleaning to the intricacies of a particular application and the data for that application.  Although this is easy to do, it makes it difficult to share and re-use data cleaning procedures for similar tasks in the future, or share within an organization.  In these cases, tools to help prevent overfitting, similar to regularization in machine learning, are needed to help generalize data cleaning workflows into reusable components.  High level scripting languages can help in this regard, but automated tools are also needed.




* Here’s a general system/interactive loop that is pie in the sky that would workfor the 99% cases.
    * Data cleaning is user and context dependent.  So much relies on the developer’s, or downstream user's expert knowledge that interactive solutions are needed.    
    * In fact, in some cases, the data cleaning process is divorced from the users using the data cleaning, which is surprising given the context-sensitive nature.
    * trying cleaning procedures, algorithms, tuning parameters out, and evaluating the results through visual inspection of computed metrics, or the cleaned data
    * Needs highly interactive systems, and often visual systems for domain experts
    * Figure 1 shows a highlevel loop
    * Only when combined into a single system does it make sense
    * This motivates a set of interesting technical problems that we outline below.

\fi
