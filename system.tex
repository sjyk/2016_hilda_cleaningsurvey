\section{Discussion and Directions for Future Research}\label{sec:future}


\begin{figure}
  \centering
  \includegraphics[width=.8\columnwidth]{datafigs/arch}
  \caption{Something}
  \label{f:arch}
\end{figure}

Our survey results highlighted examples of many places in the data cleaning and analysis process that introduce friction that reduces the ability to rapidly cleanse the data and build useful downstream applications.
Our primary finding reiterates a well understood knowledge that data cleaning is highly contextual and depends on the application goals as well as the expertise of the application developer.  In most cases, the developer is the most important ``human in the (data cleaning) loop''.  However, contrary to this understanding, we found that the data cleaning process is often performed by an IT or separate organization that is largely divorced from the developers using the cleaned data.  Due to this chasm, the ability to try different combinations cleaning procedures and tune their parameters is reduced --- we find that feedback about the data cleaning process must wait until the downstream application developer (most commonly visually) inspects the application results, thus further hindering interactivity.  These issues motivate a set of interesting techincal problems that hopefully lubricate the interactive loop.

As highlighted in the top half of Figure~\ref{f:arch}, changes to the cleaning process occur based on observations made at the end of the analysis pipeline.  


\noindent\textbf{Developing High-level Language for Domain Experts:}  Acknowledge there is a bias in our user study population, however other studies have shown that users donstream from data cleaning operations can often be non-technical domain experts.  Need to bridge the high level specification of the data cleaning goals --- providing exapmles of deduplication goals, what outliers are --- with the techincal organization that is tasked with performing data cleaning at scale.

\noindent\textbf{Usability:} Often times the domain experts are not techincal experts, and there is a need to map the high-level language into the visual interactive domain.  We have seen success from intergrating a domain specific language and visual interactions  in specialized data cleaning domains such as text extraction~\cite{wrangler}.  Similarly agreementmaker work.

long pipeline from cleaning and output, so insert inspection.  

\noindent\textbf{Application-oriented Cleaning:}  A reoccuring theme across our survey participants was the observation that the data cleaning was driven by the needs of the downstream application, and in mayn cases the lack of a rapid feedback loop resulted in the appearance of data cleaning happening in a vacuum.  This highlights the need for pay-as-you-go solutions~\cite{dataspaces} that provide a voice to the application developer about the types of data cleaning that are needed.  Automated solutions are also needed. For example, systems such as SampleClean~\cite{} and ActiveClean~\cite{} suggest individual data items to clean based on the donwnstream query or machine learning model.   Similar ideas are useful.

\noindent\textbf{Human-Computer Symbiosis:}  In other words {\it let experts do what they do best, and let machines do the rest}.
    * Hyperparameter tuning, however visual inspection by manually tuning each or combinations of parameters is both slow and and error-prone.  Automated hyperparemeter tuning (REF evan)
    * String value munging, either deduplication, extraction, fixes.  Existing tools are often not sufficient and user intervention (REF Wrangler) is necessary.  However even in this settings, ways to generate suggestions and show them is better than users picking operations to perform.
    * active learning is used, but only for crowd workers that fit inside an operator good progress folks.
    * Opporutinity let the expert be the human in the loop.  The data analyst IS the human in the loop.
    * Let humans do what they do best, machines to the rest.


\noindent\textbf{Testing and Debugging:} In order to develop automated optimization and tuning procedures, we need a metric to optimize towards.  We have seen how this can be challenging --- participants noted that their measure of data cleaning effectiveness was whether or not the downstream process compiled and ran.  
Along these lines, as data-cleaning researchers, there is a need for a ``dirtiness'' benchmark analgous to industry standard transaction and analytical data processing benchmarks.  Existing evaluations are often based on synthetically generated errors and are susceptible to overfitting to synthetic cases or specialized types of errors evaluated in isolation.  Such a benchmark needs to support cases when there is and is not a ground truth dataset.

\noindent\textbf{Combating Overfitting:} Along the lines of testing and debugging, we find evidence of overfitting the data cleaning procedures to the specific instance of the downstream applicationi.  For instance, {\color{red}{EXAMPLE}}.  This can lead to overly complex, brittle data cleaning pipelines that are hard to share with other applications and may not apply to future evolutions of the application.  APPLY SANJAY



In summary, we are in the early days of highly interactive data cleaning and preparation and we are optimistic about the improvement that can be made by bringing user interactivity into the process.


\if{0}
 
* A language or languages for data cleaning.  An operation, such as deduplication involves a large number of possible algorithms or sytems techinques, ranging from crowdsourced solsutions(REF Anhai), machine learning-based fuzzy matching(REF).  It is often easier for a user to provide correct results rather than.
* Be careful about survey biased towards technical people, not those that use GUIs. 
* Usability: Tied with the language are a set of interactions to match the language operations (REF joe’s CIDR paper).  This is admittedly an amorpheous topic.  (REF agreementmaker work)
* Goal oriented cleaning.  General data cleaning is rarely needed, and pay-as-you-go solutions(REF data spaces) are needed in most applications.   This begs the qusetion for hminimizing expert involvement.   Solutions such as sampleclean are oriented towards specializing the human effort towards porudictive tasks that maximize the cleaning for a particular application.  
* Automation. 
    * Hyperparameter tuning, however visual inspection by manually tuning each or combinations of parameters is both slow and and error-prone.  Automated hyperparemeter tuning (REF evan)
    * String value munging, either deduplication, extraction, fixes.  Existing tools are often not sufficient and user intervention (REF Wrangler) is necessary.  However even in this settings, ways to generate suggestions and show them is better than users picking operations to perform.
    * active learning is used, but only for crowd workers that fit inside an operator good progress folks.
    * Opporutinity let the expert be the human in the loop.  The data analyst IS the human in the loop.
    * Let humans do what they do best, machines to the rest.
* Testing and debugging cleaning.  Data cleaning does not live within a vacuum.  It is often motivated by a set of desired applications, and .  How can it be tested?  For example, what effects will introducing or modiying a data cleaning procedure have on the rest of the dataset, or the downstream application?  For example, in our survey several respondents noted that their evaluation was based on whether the downstream applicaiton compiles or not.
    * Reference Self eval idea.  long pipeline from cleaning and output, so insert inspection.  
    * Reference benchmarking work and systematic ways to evaluate the efficacy of an approach when there is ground truth and when there is no ground truth.
* (SANJAY) Overfitting and sharing.   It is easy to overfit the data cleaning to the intricacies of a particular application and the data for that application.  Although this is easy to do, it makes it difficult to share and re-use data cleaning procedures for similar tasks in the future, or share within an organization.  In these cases, tools to help prevent overfitting, similar to regularization in machine learning, are needed to help generalize data cleaning workflows into reusable components.  High level scripting languages can help in this regard, but automated tools are also needed.




* Here’s a general system/interactive loop that is pie in the sky that would workfor the 99% cases.
    * Data cleaning is user and context dependent.  So much relies on the developer’s, or downstream user's expert knowledge that interactive solutions are needed.    
    * In fact, in some cases, the data cleaning process is divorced from the users using the data cleaning, which is surprising given the context-sensitive nature.
    * trying cleaning procedures, algorithms, tuning parameters out, and evaluating the results through visual inspection of computed metrics, or the cleaned data
    * Needs highly interactive systems, and often visual systems for domain experts
    * Figure 1 shows a highlevel loop
    * Only when combined into a single system does it make sense
    * This motivates a set of interesting technical problems that we outline below.

\fi
