\section{Data Cleaning Survey}
Between April 2015 and Dec 2015, we conducted two surveys (N=29) and 4 interviews with engineers, data analysts, and scientists who self-reported that they directly work with data in their organizations.

\subsection{Methods}
The surveys consisted of a series of quantitative questions about tool/language preference and job description, and several open-ended questions about the participants' organization's data management challenges. The interviews mirrored the surveys and were conducted and recorded by their authors. 
It is important to note that we conducted two separate surveys to ensure that our survey questions were properly calibrated. The first survey was conducted in June 2015, and we collected (N=8) responses to a preliminary set of 18 questions, and we conducted 4 in-person/phone interviews based on the same questions. Using the results of this survey, and revising the questions, we conducted a second survey (N=21) with more specific language on the questions. 
Most of the participants were contacted personally by the authors and were often acquaintances of the authors. However, the authors took care to ensure that the participants were not informed of any of the quantitative hypotheses or conclusions of the study before taking the survey. 
Some participants were reached through forums frequented by data analysts\footnote{http://reddit.com/r/datascience}, and all participants had the option to take the survey anonymously.

The questions and the data for the survey are available at: \url{http://TODO}

\subsubsection{Participant Demographics}
We asked all participants to self-report a job description. Some of the participants included specific names and products, while others chose to describe their jobs without identifying information. We categorized the participants by their reported organization size and their stated data analysis goal (if any). Based on the reported data, the sample was evenly split between large organizations and small organizations; although a large number of participants gave vague responses that could not be verified.

\subsection{Themes}
Interactive vs noninteractive

How does debugging/validating correct cleaning get done?

The 99 percent

Overfitting the cleaning
