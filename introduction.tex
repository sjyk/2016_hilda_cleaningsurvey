% !TEX root = main.tex
\section{Introduction}
As large-scale data sources have become increasingly numerous and easy to acquire, the challenges associated with integrating and preparing them for analysis have also multiplied.
Consequently, a necessary step in modern data analysis is \textit{data cleaning}, wherein issues caused by incorrect, missing, and duplicate data are resolved.
In 2012, Kandel et al. surveyed a number of data analysts and found that over 80\% of their time is spent cleaning their data during analysis~\cite{kandel2012}.

Traditionally, data cleaning routines have been viewed as static components that fit into data integration or Extract-Transform-Load (ETL) pipelines and are executed once on new data entering the system.~\cite{apachefalcon, informatica, talend, nadeef}.
However, this perspective fails to take into account the fact that data cleaning is frequently an iterative process tailored to the requirements of a specific analysis task.
As a result, several systems have been developed recently to support the iterative specification and refinement of data cleaning workflows~\cite{trifacta, 2011-wrangler, openrefine, wisteria}.

Because such human-in-the-loop systems are inherently interactive, their design and implementation presents novel challenges to the data cleaning community.
For example, usability becomes a key concern, as data analysts must repeatedly instantiate, modify, and execute data cleaning programs.
Truly usable systems will need improve over the state of the art along a number of dimensions: increasing the automation of as many cleaning operations as possible and providing crowdsourcing support for complex operations that cannot be fully automated, driving the performance of automated and crowdsourced operations to interactive latencies, providing debugging mechanisms for analysts to understand and react to the output of cleaning operations, et cetera.

The data cleaning community has long studied abstractions for modeling data error and designing large scale cleaning systems, and we believe the time is ripe to focus attention towards usability and interactivity.%---this presents a significant opportunity for technical problems and impact to data cleaning researchers.
%To avoid building feature-rich but ultimately low-impact systems, however, it is important to guide new research in directions that will be valuable for actual practitioners of data cleaning.
To better understand this problem space, we provide guidance through interviews and surveys with users that perform real-world data cleaning. %In this paper, we provide such guidance, supported by concrete evidence from real-world data cleaners.
We first present our survey of a variety of engineers, data analysts and scientists who frequently work with data, then offer a qualitative analysis of the survey results that exposes several important themes in the workflows, methodologies, and challenges faced by practitioners today.
Driven by these insights, we present an ambitious vision for the future of data cleaning systems, focused on a set of pragmatic challenges with the greatest opportunity to improve current process for the many data analysts who spend the majority of their time struggling with manipulating dirty data.

In summary, our contributions are as follows:
\begin{itemize}
\item A survey of N=29 industry users of data analysis software that highlights the strengths and limitations of existing data cleaning workflows (Section~\ref{sec:survey}).
\item A qualitative analysis of the survey results that highlights 3 key themes of modern data cleaning: (1) its iterative nature, (2) the relationship between the analytics and infrastructure staff who perform it, and (3) the difficulties inherent in its validation (Section~\ref{sec:themes}).
\item A proposal for future research that addresses these themes within a unified framework for interactive data cleaning (Section~\ref{sec:future}).
\end{itemize}
